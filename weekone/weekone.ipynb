{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/week-one/\"\n",
    "train_filename, test_filename, macro_filename = \"X_train.csv\", \"X_test.csv\", \"macro.csv\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(DATA_PATH, train_filename), parse_dates=['timestamp'])\n",
    "test = pd.read_csv(os.path.join(DATA_PATH, test_filename), parse_dates=['timestamp'])\n",
    "macro = pd.read_csv(os.path.join(DATA_PATH, macro_filename), parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape, test.shape, macro.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions used for the preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCorrelation(data):\n",
    "    corr_data = data.copy()\n",
    "    names = list(corr_data.columns)\n",
    "    correlations = corr_data.corr().abs()\n",
    "    fig = plt.figure(figsize=(50, 50))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "    # fig.colorbar(cax)\n",
    "    ticks = np.arange(0,len(names),1)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_yticklabels(names)\n",
    "    plt.show()\n",
    "\n",
    "def reduce(data, threshold=0.9):\n",
    "    correlations = data.corr().abs()\n",
    "    upper = correlations.where(\n",
    "        np.triu(np.ones(correlations.shape), k=1).astype(np.bool))\n",
    "    to_drop = [\n",
    "        column for column in upper.columns if any(upper[column] > threshold)\n",
    "    ]\n",
    "    return data.drop(columns=to_drop)\n",
    "\n",
    "def inpute(data, feature, verbose=False, **kwargs):\n",
    "    X = data.copy().drop(columns=[feature])\n",
    "    X = X.select_dtypes(exclude=['object'])\n",
    "    X = X.fillna(X.median())\n",
    "    y = data[feature]\n",
    "    X_train = X[~y.isna()]\n",
    "    X_test = X[y.isna()]\n",
    "    y_train = y[~y.isna()]\n",
    "\n",
    "    model = DecisionTreeRegressor(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(\"Feature: %s\" % feature)\n",
    "    filled_gaps = model.predict(X_test)\n",
    "    for i, ind in enumerate(data[feature][data[feature].isna()].index):\n",
    "        data.at[ind, feature] = filled_gaps[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add macro data, only select features which are strongly correlated with the housing price. (This was compute offline). And extra feature, the year of sale is also added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "macro_features = ['timestamp', 'bandwidth_sports', 'fixed_basket', 'cpi', 'gdp_annual_growth',\n",
    "                  'salary', 'deposits_value', 'load_of_teachers_school_per_teacher',\n",
    "                  'turnover_catering_per_cap', 'gdp_deflator', 'gdp_annual']\n",
    "plotCorrelation(macro)\n",
    "macro = macro[macro_features]\n",
    "# macro = macro.fillna(macro.median())\n",
    "data = pd.merge_ordered(data, macro, on='timestamp', how='left')\n",
    "data['year'] = pd.DatetimeIndex(data['timestamp']).year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out target and features and exclude categorical features from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = data.copy()[\"price_doc\"]\n",
    "data.drop(['id', 'price_doc'], axis=1, inplace=True)\n",
    "X = data.copy()\n",
    "\n",
    "# Take only numeric data for now\n",
    "X = X.select_dtypes(exclude=['object'])\n",
    "X.drop(columns=[\"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensionality by removing strongly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrelation(X)\n",
    "X = reduce(X, threshold=0.95)\n",
    "plotCorrelation(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add categorical features using one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features would be best described with ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ecology'] = data['ecology'].map({'excellent':4,'good':3,'satisfactory':2,'poor':1,'no data':np.nan})\n",
    "X['ecology'] = data['ecology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['object']).drop(columns=['sub_area', 'product_type']).columns:\n",
    "    data[column] = data[column].map({'yes':1, 'no':0})\n",
    "    X[column] = data[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X,pd.get_dummies(data.select_dtypes(include=['object']))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a basic decision tree regressor to predict missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in X.columns[X.isna().any() == True]:\n",
    "    X = inpute(X, column, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers. Note: removing these values seemed to strongly reduce the accuracy of the model and in places gave infinities for the predictions. For these reasons, the outliers were kept in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.DataFrame(dict([(column,abs(stats.zscore(X[column]))) for column in X.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[~((z > 5).sum(axis=1) > 5)]\n",
    "print(\"Data shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from weekone_models import models\n",
    "\n",
    "models = {\n",
    "    \"ridge\": {\n",
    "        'model': sklearn.linear_model.Ridge(),\n",
    "        'param_grid': {\n",
    "            'ridge__alpha': np.logspace(2, 6, 10)\n",
    "        }\n",
    "    },\n",
    "    \"lasso\": {\n",
    "        'model': sklearn.linear_model.Lasso(),\n",
    "        'param_grid': {\n",
    "            'lasso__alpha': np.logspace(-5, 1, 10)\n",
    "        }\n",
    "    },\n",
    "    \"elasticnet\": {\n",
    "        'model': sklearn.linear_model.ElasticNet(),\n",
    "        'param_grid': {\n",
    "            'elasticnet__alpha': np.logspace(-5, 1, 10)\n",
    "        }\n",
    "    },\n",
    "    \"linearsvr\": {\n",
    "        'model': sklearn.svm.LinearSVR(),\n",
    "        'param_grid': {\n",
    "            'linearsvr__C': np.logspace(-5, 0, 5)\n",
    "        }\n",
    "    },\n",
    "    \"decisiontreeregressor\": {\n",
    "        'model': DecisionTreeRegressor(),\n",
    "        'param_grid' : {\n",
    "            'decisiontreeregressor__max_depth': np.logspace(0, 1.3, 10, dtype=int),\n",
    "            'decisiontreeregressor__min_samples_leaf': np.logspace(2, 3, 5, dtype=int)\n",
    "        }\n",
    "    },\n",
    "    \"adaboostregressor\": {\n",
    "        'model': sklearn.ensemble.AdaBoostRegressor(DecisionTreeRegressor(max_depth=3)),\n",
    "        'param_grid': {\n",
    "            'adaboostregressor__n_estimators': np.logspace(0, 3, 10, dtype=int)\n",
    "        }\n",
    "    },\n",
    "    \"mlpregressor\": {\n",
    "        'model': MLPRegressor(),\n",
    "        'param_grid': {\n",
    "            'mlpregressor__alpha': np.logspace(-5,-1,10)\n",
    "        }\n",
    "    },\n",
    "    \"randomforestregressor\": {\n",
    "        'model': sklearn.ensemble.RandomForestRegressor(),\n",
    "        'param_grid': {\n",
    "            'n_estimators': np.logspace(1, 3, 20)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over each model defined in the dictionary and find optimal hyperparameters through RandomizedSearchCV (Previously GridSearchCV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(\"Performing search for %s model\" % model)\n",
    "    pipeline = make_pipeline(StandardScaler(), models[model]['model'])\n",
    "\n",
    "    param_grid = models[model]['param_grid']\n",
    "\n",
    "    gscv = RandomizedSearchCV(\n",
    "        pipeline, param_grid, n_jobs=-1,\n",
    "        scoring='neg_root_mean_squared_error', verbose=1, cv=5,\n",
    "        refit='best_index_', n_iter=20\n",
    "    )\n",
    "    gscv.fit(X, np.log1p(y.loc[X.index]))\n",
    "    models[model]['best_estimator'] = gscv.best_estimator_\n",
    "    models[model]['best_score'] = gscv.best_score_\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a voting regressor to combine all of the models into an ensemble. Each model is weighted by its accuracy during the optimisation process. This ensemble is then fitted over the complete data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    VotingRegressor(\n",
    "        estimators=[(model, models[model]['best_estimator'].steps[1][1]) for model in models] + [],\n",
    "        weights=[1/abs(models[model]['best_score']) for model in models],\n",
    "        n_jobs=-1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,np.log1p(y.loc[X.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.steps[1][1].estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(\n",
    "    model.predict(X),\n",
    "    np.log1p(y.loc[X.index]),\n",
    "    squared=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ecology'] = test['ecology'].map({'excellent':4,'good':3,'satisfactory':2,'poor':1,'no data':np.nan})\n",
    "test = pd.merge_ordered(test, macro.fillna(macro.median()), on='timestamp', how='left')\n",
    "for column in test.select_dtypes(include=['object']).drop(columns=['sub_area', 'product_type']).columns:\n",
    "    test[column] = test[column].map({'yes':1, 'no':0})\n",
    "X_predict = pd.concat([test.copy(),pd.get_dummies(test.select_dtypes(include=['object']))], axis=1)\n",
    "for column in X.columns:\n",
    "    if column not in X_predict:\n",
    "        X_predict[column] = 0\n",
    "X_predict = X_predict[X.columns]\n",
    "for column in X_predict.columns[X_predict.isna().any() == True]:\n",
    "    X_predict = inpute(X_predict, column, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.expm1(model.predict(X_predict))\n",
    "predictions = pd.DataFrame(predictions, columns=[\"price_doc\"])\n",
    "predictions = pd.concat([test['id'], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(os.path.join(DATA_PATH, \"predictions.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv = GradientBoostingRegressor(), \n",
    "        {\n",
    "            'max_depth': np.linspace(10, 30, 10, dtype=int),\n",
    "            'min_samples_leaf': np.linspace(100, 200, 10, dtype=int),\n",
    "            'n_estimators': np.linspace(300,400, 10, dtype=int),\n",
    "            'learning_rate': np.linspace(0.01, 0.03, 10),\n",
    "        }, \n",
    "        n_jobs=-1, n_iter=5, verbose=2, cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        refit='best_index_'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.fit(X,np.log1p(y.loc[X.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best estimator copied into the code so as to reduce the number of repeats of the CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = sklearn.ensemble.GradientBoostingRegressor(\n",
    "    alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
    "    init=None, learning_rate=0.018888888888888886,\n",
    "    loss='ls', max_depth=16, max_features=None,\n",
    "    max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None, min_samples_leaf=188,\n",
    "    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=400, n_iter_no_change=None,\n",
    "    presort='deprecated', random_state=None,\n",
    "    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "    verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator.fit(X,np.log1p(y.loc[X.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE using the GradientBoostedRegressor is significantly lower than the ensemble predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(\n",
    "    best_estimator.predict(X),\n",
    "    np.log1p(y.loc[X.index]),\n",
    "    squared=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.expm1(gscv.best_estimator_.predict(X_predict))\n",
    "predictions = pd.DataFrame(predictions, columns=[\"price_doc\"])\n",
    "predictions = pd.concat([test['id'], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(os.path.join(DATA_PATH, \"predictions.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.ensemble.RandomForestRegressor(max_depth=20, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,np.log1p(y.loc[X.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(model.feature_importances_,X.columns).sort_values(ascending=False).iloc[:10].plot.bar()\n",
    "plt.ylabel(\"Feature importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes:\n",
    "- Adding PCA to the pipeline did not improve accuracy. Decision trees do not benefit from such transformations so this step was redundant and just increased compute time.\n",
    "- Similarly, adding the macro data in its complete form served only increase the complexity fo the model, without improving the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
