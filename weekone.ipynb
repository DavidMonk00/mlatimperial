{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/week-one/\"\n",
    "train_filename, test_filename, macro_filename = \"X_train.csv\", \"X_test.csv\", \"macro.csv\"\n",
    "\n",
    "data = pd.read_csv(os.path.join(DATA_PATH, train_filename), parse_dates=['timestamp'])\n",
    "test = pd.read_csv(os.path.join(DATA_PATH, test_filename), parse_dates=['timestamp'])\n",
    "macro = pd.read_csv(os.path.join(DATA_PATH, macro_filename), parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21329, 292) (9142, 291) (2484, 100)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, test.shape, macro.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful functions used for the preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(data, threshold=0.9):\n",
    "    correlations = data.corr().abs()\n",
    "    upper = correlations.where(\n",
    "        np.triu(np.ones(correlations.shape), k=1).astype(np.bool))\n",
    "    to_drop = [\n",
    "        column for column in upper.columns if any(upper[column] > threshold)\n",
    "    ]\n",
    "    return data.drop(columns=to_drop)\n",
    "\n",
    "def inpute(data, feature, verbose=False, **kwargs):\n",
    "    X = data.copy().drop(columns=[feature])\n",
    "    X = X.select_dtypes(exclude=['object'])\n",
    "    X = X.fillna(X.median())\n",
    "    y = data[feature]\n",
    "    X_train = X[~y.isna()]\n",
    "    X_test = X[y.isna()]\n",
    "    y_train = y[~y.isna()]\n",
    "\n",
    "    model = DecisionTreeRegressor(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print(\"Feature: %s\" % feature)\n",
    "    filled_gaps = model.predict(X_test)\n",
    "    for i, ind in enumerate(data[feature][data[feature].isna()].index):\n",
    "        data.at[ind, feature] = filled_gaps[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out target and features and exclude categorical features from training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add macro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "macro_features = ['timestamp', 'bandwidth_sports', 'fixed_basket', 'cpi', 'gdp_annual_growth',\n",
    "                  'salary', 'deposits_value', 'load_of_teachers_school_per_teacher',\n",
    "                  'turnover_catering_per_cap', 'gdp_deflator', 'gdp_annual']\n",
    "macro = macro[macro_features]\n",
    "# macro = macro.fillna(macro.median())\n",
    "data = pd.merge_ordered(data, macro, on='timestamp', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = data.copy()[\"price_doc\"]\n",
    "data.drop(['id', 'price_doc'], axis=1, inplace=True)\n",
    "# self.X = pd.merge_ordered(\n",
    "#     self.data.copy(), self.macro.copy(), on='timestamp', how='left')\n",
    "X = data.copy()\n",
    "# self.X.fillna(self.X.median(), inplace=True)\n",
    "\n",
    "# Take only numeric data for now\n",
    "X = X.select_dtypes(exclude=['object'])\n",
    "X.drop(columns=[\"timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensionality by removing strongly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reduce(X, threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add categorical features using one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features would be best described with ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ecology'] = data['ecology'].map({'excellent':4,'good':3,'satisfactory':2,'poor':1,'no data':np.nan})\n",
    "X['ecology'] = data['ecology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['object']).drop(columns=['sub_area', 'product_type']).columns:\n",
    "    data[column] = data[column].map({'yes':1, 'no':0})\n",
    "    X[column] = data[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X,pd.get_dummies(data.select_dtypes(include=['object']))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (21329, 323)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a basic decision tree regressor to predict missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in X.columns[X.isna().any() == True]:\n",
    "    X = inpute(X, column, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (21329, 323)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.DataFrame(dict([(column,abs(stats.zscore(X[column]))) for column in X.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (21329, 323)\n"
     ]
    }
   ],
   "source": [
    "# X = X[~((z > 5).sum(axis=1) > 5)]\n",
    "print(\"Data shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weekone_models import models\n",
    "\n",
    "# models = {\n",
    "#     \"ridge\": {\n",
    "#         'model': sklearn.linear_model.Ridge(),\n",
    "#         'param_grid': {\n",
    "#             'ridge__alpha': np.logspace(2, 6, 10)\n",
    "#         }\n",
    "#     },\n",
    "#     # \"LinearRegression\": LinearRegression(),\n",
    "#     \"lasso\": {\n",
    "#         'model': sklearn.linear_model.Lasso(),\n",
    "#         'param_grid': {\n",
    "#             'lasso__alpha': np.logspace(-5, 1, 10)\n",
    "#         }\n",
    "#     },\n",
    "#     \"elasticnet\": {\n",
    "#         'model': sklearn.linear_model.ElasticNet(),\n",
    "#         'param_grid': {\n",
    "#             'elasticnet__alpha': np.logspace(-5, 1, 10)\n",
    "#         }\n",
    "#     },\n",
    "# #     \"linearsvr\": {\n",
    "# #         'model': sklearn.svm.LinearSVR(),\n",
    "# #         'param_grid': {\n",
    "# #             'linearsvr__C': np.logspace(-5, 0, 5)\n",
    "# #         }\n",
    "# #     },\n",
    "#     # \"BayesianRidge\": BayesianRidge(),\n",
    "#     # # \"ARDRegression\": ARDRegression(),\n",
    "#     # \"NuSVR\": NuSVR(),\n",
    "#     # # \"KernelRidge\": KernelRidge(),\n",
    "#     # # \"GaussianProcessRegressor\": GaussianProcessRegressor(),\n",
    "#     \"decisiontreeregressor\": {\n",
    "#         'model': DecisionTreeRegressor(),\n",
    "#         'param_grid' : {\n",
    "#             'decisiontreeregressor__max_depth': np.logspace(0, 1.3, 10, dtype=int),\n",
    "#             'decisiontreeregressor__min_samples_leaf': np.logspace(2, 3, 5, dtype=int)\n",
    "#         }\n",
    "#     },\n",
    "#     \"adaboostregressor\": {\n",
    "#         'model': sklearn.ensemble.AdaBoostRegressor(DecisionTreeRegressor(max_depth=3)),\n",
    "#         'param_grid': {\n",
    "#             'adaboostregressor__n_estimators': np.logspace(0, 3, 10, dtype=int)\n",
    "#         }\n",
    "#     },\n",
    "# #     \"mlpregressor\": {\n",
    "# #         'model': MLPRegressor(),\n",
    "# #         'param_grid': {\n",
    "# #             'mlpregressor__alpha': np.logspace(-5,-1,10)\n",
    "# #         }\n",
    "# #     }\n",
    "#     # \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor()\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#     print(\"Performing search for %s model\" % model)\n",
    "#     pipeline = make_pipeline(StandardScaler(), models[model]['model'])\n",
    "\n",
    "#     param_grid = models[model]['param_grid']\n",
    "\n",
    "#     gscv = RandomizedSearchCV(\n",
    "#         pipeline, param_grid, n_jobs=-1,\n",
    "#         scoring='neg_root_mean_squared_error', verbose=1, cv=5,\n",
    "#         refit='best_index_'\n",
    "#     )\n",
    "#     gscv.fit(X, np.log1p(y.loc[X.index]))\n",
    "#     models[model]['best_estimator'] = gscv.best_estimator_\n",
    "#     models[model]['best_score'] = gscv.best_score_\n",
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_pipeline(\n",
    "#     StandardScaler(),\n",
    "#     VotingRegressor(\n",
    "#         estimators=[(model, models[model]['best_estimator'].steps[1][1]) for model in models] + [],\n",
    "#         weights=[1/abs(models[model]['best_score']) for model in models],\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X,np.log1p(y.loc[X.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.steps[1][1].estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_squared_error(\n",
    "#     model.predict(X),\n",
    "#     np.log1p(y.loc[X.index]),\n",
    "#     squared=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmonk/anaconda3/envs/ml/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:994: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4483b06bba02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mX_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minpute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-f09f0c533c37>\u001b[0m in \u001b[0;36minpute\u001b[0;34m(data, feature, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "test['ecology'] = test['ecology'].map({'excellent':4,'good':3,'satisfactory':2,'poor':1,'no data':np.nan})\n",
    "test = pd.merge_ordered(test, macro.fillna(macro.median()), on='timestamp', how='left')\n",
    "for column in test.select_dtypes(include=['object']).drop(columns=['sub_area', 'product_type']).columns:\n",
    "    test[column] = test[column].map({'yes':1, 'no':0})\n",
    "X_predict = pd.concat([test.copy(),pd.get_dummies(test.select_dtypes(include=['object']))], axis=1)\n",
    "for column in X.columns:\n",
    "    if column not in X_predict:\n",
    "        X_predict[column] = 0\n",
    "X_predict = X_predict[X.columns]\n",
    "for column in X_predict.columns[X_predict.isna().any() == True]:\n",
    "    X_predict = inpute(X_predict, column, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = np.expm1(model.predict(X_predict))\n",
    "# predictions = pd.DataFrame(predictions, columns=[\"price_doc\"])\n",
    "# predictions = pd.concat([test['id'], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.to_csv(os.path.join(DATA_PATH, \"predictions.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = macro.fillna(macro.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pd.merge_ordered(data, macro, on='timestamp', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv = make_pipeline(StandardScaler(), RandomizedSearchCV(\n",
    "        GradientBoostingRegressor(), \n",
    "        {\n",
    "            'max_depth': np.linspace(10, 30, 10, dtype=int),\n",
    "            'min_samples_leaf': np.linspace(100, 200, 10, dtype=int),\n",
    "            'n_estimators': np.linspace(300,400, 10, dtype=int),\n",
    "            'learning_rate': np.linspace(0.01, 0.03, 10),\n",
    "        }, \n",
    "        n_jobs=-1, n_iter=5, verbose=2, cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        refit='best_index_'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  25 | elapsed: 41.4min remaining:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed: 45.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('randomizedsearchcv',\n",
       "                 RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                                    estimator=GradientBoostingRegressor(alpha=0.9,\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        criterion='friedman_mse',\n",
       "                                                                        init=None,\n",
       "                                                                        learning_rate=0.1,\n",
       "                                                                        loss='ls',\n",
       "                                                                        max_depth=3,\n",
       "                                                                        max_features=None,\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        min_impurity_...\n",
       "       0.02111111, 0.02333333, 0.02555556, 0.02777778, 0.03      ]),\n",
       "                                                         'max_depth': array([10, 12, 14, 16, 18, 21, 23, 25, 27, 30]),\n",
       "                                                         'min_samples_leaf': array([100, 111, 122, 133, 144, 155, 166, 177, 188, 200]),\n",
       "                                                         'n_estimators': array([300, 311, 322, 333, 344, 355, 366, 377, 388, 400])},\n",
       "                                    pre_dispatch='2*n_jobs', random_state=None,\n",
       "                                    refit='best_index_',\n",
       "                                    return_train_score=False,\n",
       "                                    scoring='neg_root_mean_squared_error',\n",
       "                                    verbose=2))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.fit(X,np.log1p(y.loc[X.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.46517701022322966"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.steps[1][1].best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.018888888888888886,\n",
       "                          loss='ls', max_depth=16, max_features=None,\n",
       "                          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                          min_impurity_split=None, min_samples_leaf=188,\n",
       "                          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                          n_estimators=400, n_iter_no_change=None,\n",
       "                          presort='deprecated', random_state=None,\n",
       "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.steps[1][1].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = GradientBoostingRegressor(\n",
    "    alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
    "    init=None, learning_rate=0.018888888888888886,\n",
    "    loss='ls', max_depth=16, max_features=None,\n",
    "    max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None, min_samples_leaf=188,\n",
    "    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=400, n_iter_no_change=None,\n",
    "    presort='deprecated', random_state=None,\n",
    "    subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "    verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.018888888888888886,\n",
       "                          loss='ls', max_depth=16, max_features=None,\n",
       "                          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                          min_impurity_split=None, min_samples_leaf=188,\n",
       "                          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                          n_estimators=400, n_iter_no_change=None,\n",
       "                          presort='deprecated', random_state=None,\n",
       "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator.fit(X,np.log1p(y.loc[X.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37668506581859956"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(\n",
    "    best_estimator.predict(X),\n",
    "    np.log1p(y.loc[X.index]),\n",
    "    squared=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.expm1(gscv.best_estimator_.predict(X_predict))\n",
    "predictions = pd.DataFrame(predictions, columns=[\"price_doc\"])\n",
    "predictions = pd.concat([test['id'], predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(os.path.join(DATA_PATH, \"predictions.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
